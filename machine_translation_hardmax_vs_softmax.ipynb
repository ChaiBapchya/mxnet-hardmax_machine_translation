{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation\n",
    "It is a Natural Language processing task of converting sequence of text from one language to another.\n",
    "It uses Encoder-Decoder and Attention mechanism for achieving the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, rnn\n",
    "\n",
    "PAD, BOS, EOS = '<pad>', '<bos>', '<eos>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Specify the type of dataset and the file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_num=2\n",
    "file_path1 = 'fr-en-small.txt'\n",
    "file_path2 = 'tiny.europarl-v7.fr-en.en'\n",
    "file_path3 = 'tiny.europarl-v7.fr-en.fr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a particular sequence, we store all the words in all the tokens to subsequently construct the dictionary.\n",
    "# Add PAD to the sequence, until the length becomes max_seq_len\n",
    "# Store the padded sequence in all_seqs\n",
    "def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):\n",
    "    all_tokens.extend(seq_tokens)\n",
    "    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)\n",
    "    all_seqs.append(seq_tokens)\n",
    "\n",
    "# Use all the tokens to construct a dictionary\n",
    "# Construct an NDArray by converting words in all sequences to indices\n",
    "def build_data(all_tokens, all_seqs):\n",
    "    vocab = text.vocab.Vocabulary(collections.Counter(all_tokens),\n",
    "                                  reserved_tokens=[PAD, BOS, EOS])\n",
    "    indices = [vocab.to_indices(seq) for seq in all_seqs]\n",
    "    return vocab, nd.array(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the data, append \"&lt;eos&gt;” at the end of the sentence，and if needed, make the length of each sequecnce equal to `max_seq_len` by adding the “&lt;pad&gt;” symbol.\n",
    "\n",
    "#### Seperation of concerns\n",
    "We create seperate dictionaries for French and English words. The index of French words is independent of the index of English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "31"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(max_seq_len):\n",
    "    # in, out are abbreviations of input and output, respectively\n",
    "    in_tokens, out_tokens, in_seqs, out_seqs = [], [], [], []\n",
    "    if(dataset_num==1):\n",
    "        with io.open(file_path1) as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            in_seq, out_seq = line.rstrip().split('\\t')\n",
    "            in_seq_tokens, out_seq_tokens = in_seq.split(' '), out_seq.split(' ')\n",
    "            if max(len(in_seq_tokens), len(out_seq_tokens)) > max_seq_len - 1:\n",
    "                continue  # If a sequence is longer than the max_seq_len after adding EOS, this example will be ignored.\n",
    "            process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)\n",
    "            process_one_seq(out_seq_tokens, out_tokens, out_seqs, max_seq_len)\n",
    "        in_vocab, in_data = build_data(in_tokens, in_seqs)\n",
    "        out_vocab, out_data = build_data(out_tokens, out_seqs)\n",
    "        return in_vocab, out_vocab, gdata.ArrayDataset(in_data, out_data)\n",
    "    else:\n",
    "        with io.open(file_path2) as f_en, io.open(file_path3) as f_fr:\n",
    "            for english_lines, french_lines in zip(f_en, f_fr):\n",
    "                in_seq = english_lines.strip()\n",
    "                out_seq = french_lines.strip()\n",
    "                in_seq_tokens, out_seq_tokens = in_seq.split(' '), out_seq.split(' ')\n",
    "                if max(len(in_seq_tokens), len(out_seq_tokens)) > max_seq_len - 1:\n",
    "                    continue  # If a sequence is longer than the max_seq_len after adding EOS, this example will be ignored.\n",
    "                process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)\n",
    "                process_one_seq(out_seq_tokens, out_tokens, out_seqs, max_seq_len)\n",
    "            in_vocab, in_data = build_data(in_tokens, in_seqs)\n",
    "            out_vocab, out_data = build_data(out_tokens, out_seqs)\n",
    "        return in_vocab, out_vocab, gdata.ArrayDataset(in_data, out_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the maximum length of the sequence to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "181"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [137.  24.   5.  82.   3.   1.   1.]\n",
       " <NDArray 7 @cpu(0)>, \n",
       " [135.   5.   6.  87.   3.   1.   1.]\n",
       " <NDArray 7 @cpu(0)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = 7\n",
    "in_vocab, out_vocab, dataset = read_data(max_seq_len)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder with Attention Mechanism\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Encoder uses word embedding layer to obtain a feature index from the word index of the input language and then input it into a multi-level gated recurrent unit (GRU). \n",
    "\n",
    "\n",
    "Gluon's `rnn.GRU` instance also returns the multi-layer hidden states of the output and final time steps after forward calculation. \n",
    "Here, the output refers to the hidden state of the hidden layer of the last layer at each time step, and it does not involve output layer calculation. The attention mechanism uses these output as key items and value items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "165"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 drop_prob=0, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=drop_prob)\n",
    "        print(\"Encoder initialized\")\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # The input shape is (batch size, number of time steps). Change the example dimension and time step dimension of the output.\n",
    "        embedding = self.embedding(inputs).swapaxes(0, 1)\n",
    "        return self.rnn(embedding, state)\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create mini-batch sequence input with a batch size of 4 and 7 time steps. We assume the number of hidden layers of the gated recurrent unit is 2 and the number of hidden units is 16. The output shape returned by the encoder after performing forward calculation on the input is (number of time steps, batch size, number of hidden units). The shape of the multi-layer hidden state of the gated recurrent unit in the final time step is (number of hidden layers, batch size, number of hidden units). For the gated recurrent unit, the `state` list contains only one element, which is the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "166"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7, 4, 16), (2, 4, 16))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.initialize()\n",
    "output, state = encoder(nd.zeros((4, 7)), encoder.begin_state(batch_size=4))\n",
    "output.shape, state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism\n",
    "\n",
    "`flatten` option for `Dense` instance : \n",
    "When the input dimension is greater than 2, by default, the `Dense` instance will treat all dimensions other than the first dimension (example dimension) as feature dimensions that require affine transformation, and will automatically convert the input into a two-dimensional matrix with rows of behavioral examples and columns of features.\n",
    "\n",
    "Here, we set the `flatten` option to `False` so that the fully connected layer only performs affine transformation on the last dimension of the input, therefore, only the last dimension of the output shape becomes the number of outputs of the fully connected layer, i.e. 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = nn.Dense(2, flatten=False)\n",
    "dense.initialize()\n",
    "dense(nd.zeros((3, 5, 7))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the function $a$ in the Attention Mechanism to transform the concatenated input through a multilayer perceptron with a single hidden layer. The input of the hidden layer is a one-to-one concatenation between the hidden state of the decoder and the hidden state of the encoder on all time steps, which uses tanh as the activation function. The number of outputs of the output layer is 1. Neither `Dense` instance use a bias and they set `flatten=False`. Here, the length of the vector $\\boldsymbol{v}$ in the $a$ function definition is a hyper-parameter, i.e. `attention_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "167"
    }
   },
   "outputs": [],
   "source": [
    "def attention_model(attention_size):\n",
    "    model = nn.Sequential()\n",
    "    model.add(nn.Dense(attention_size, activation='tanh', use_bias=False,\n",
    "                       flatten=False),\n",
    "              nn.Dense(1, use_bias=False, flatten=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardmax\n",
    "Hardmax takes the input data and converts it into a corresponding one-hot encoded tensor along the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardmax(data):\n",
    "    return data.argmax(-1).one_hot(data.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs of the attention model include query items, key items, and value items. Assume the encoder and decoder have the same number of hidden units. The query item here is the hidden state of the decoder in the previous time step, with a shape of (batch size, number of hidden units); the key and the value items are the hidden states of the encoder at all time steps, with a shape of (number of time steps, batch size, number of hidden units). The attention model returns the context variable of the current time step, and the shape is (batch size, number of hidden units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "168"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attention_forward(model, enc_states, dec_state, is_hardmax):\n",
    "    dec_states = nd.broadcast_axis(\n",
    "        dec_state.expand_dims(0), axis=0, size=enc_states.shape[0])\n",
    "    enc_and_dec_states = nd.concat(enc_states, dec_states, dim=2)\n",
    "    e = model(enc_and_dec_states)  # Shape is (number of time steps, batch size, 1)\n",
    "    if(is_hardmax):\n",
    "        alpha = hardmax(e)  # Perform hardmax operationo\n",
    "    else:\n",
    "        alpha = nd.softmax(e, axis=0) # Perform hardmax operation on timestep dimension\n",
    "    return (alpha * enc_states).sum(axis=0)  # return context variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, the encoder has 10 time steps and a batch size of 4. Both the encoder and the decoder have 8 hidden units. The attention model returns a mini-batch of context vectors, and the length of each context vector is equal to the number of hidden units of the encoder. Therefore, the output shape is (4, 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "169"
    }
   },
   "outputs": [],
   "source": [
    "# seq_len, batch_size, num_hiddens = 10, 4, 8\n",
    "# model = attention_model(10)\n",
    "# model.initialize()\n",
    "# enc_states = nd.zeros((seq_len, batch_size, num_hiddens))\n",
    "# dec_state = nd.zeros((batch_size, num_hiddens))\n",
    "# attention_forward(model, enc_states, dec_state).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with Attention Mechanism\n",
    "\n",
    "We directly use the hidden state of the encoder in the final time step as the initial hidden state of the decoder. This requires that the encoder and decoder RNNs have the same numbers of layers and hidden units.\n",
    "\n",
    "In forward calculation of the decoder, we first calculate and obtain the context vector of the current time step by using the attention model introduced above. Since the input of the decoder comes from the word index of the output language, we obtain the feature expression of the input through the word embedding layer, and then concatenate the context vector in the feature dimension. We calculate the output and hidden state of the current time step through the gated recurrent unit, using the concatenated results and the hidden state of the previous time step. Finally, we use the fully connected layer to transform the output into predictions for each output word, with the shape of (batch size, output dictionary size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "170"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 attention_size, drop_prob=0, is_hardmax=True, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = attention_model(attention_size)\n",
    "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=drop_prob)\n",
    "        self.out = nn.Dense(vocab_size, flatten=False)\n",
    "        self.is_hardmax = is_hardmax\n",
    "        print(\"Decoder initialized\")\n",
    "        print(\"Hardmax used:\"+str(self.is_hardmax))\n",
    "\n",
    "    def forward(self, cur_input, state, enc_states):\n",
    "        # Use the attention mechanism to calculate the context vector.\n",
    "        c = attention_forward(self.attention, enc_states, state[0][-1], self.is_hardmax)\n",
    "        # The embedded input and the context vector are concatenated in the feature dimension.\n",
    "        input_and_c = nd.concat(self.embedding(cur_input), c, dim=1)\n",
    "        # Add a time step dimension, with 1 time step, for the concatenation of the input and the context vector.\n",
    "        output, state = self.rnn(input_and_c.expand_dims(0), state)\n",
    "        # Remove the time step dimension, so the output shape is (batch size, output dictionary size).\n",
    "        output = self.out(output).squeeze(axis=0)\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, enc_state):\n",
    "        # Directly use the hidden state of the final time step of the encoder as the initial hidden state of the decoder.\n",
    "        return enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We first implement the `batch_loss` function to calculate the loss of a mini-batch. The input of the decoder in the initial time step is the special character `BOS`. After that, the input of the decoder in a given time step is the word from the example output sequence in the previous time step, that is, teacher forcing. Also, use mask variables to avoid the impact of padding on loss function calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(encoder, decoder, X, Y, loss):\n",
    "    batch_size = X.shape[0]\n",
    "    enc_state = encoder.begin_state(batch_size=batch_size)\n",
    "    enc_outputs, enc_state = encoder(X, enc_state)\n",
    "    # Initialize the hidden state of the decoder.\n",
    "    dec_state = decoder.begin_state(enc_state)\n",
    "    # The input of decoder at the initial time step is BOS.\n",
    "    dec_input = nd.array([out_vocab.token_to_idx[BOS]] * batch_size)\n",
    "    # Use the mask variable to ignore the loss when the label is PAD.\n",
    "    mask, num_not_pad_tokens = nd.ones(shape=(batch_size,)), 0\n",
    "    l = nd.array([0])\n",
    "    for y in Y.T:\n",
    "        dec_output, dec_state = decoder(dec_input, dec_state, enc_outputs)\n",
    "        l = l + (mask * loss(dec_output, y)).sum()\n",
    "        dec_input = y  # Use teacher forcing.\n",
    "        num_not_pad_tokens += mask.sum().asscalar()\n",
    "        # When we encounter EOS, words after the sequence will all be PAD and the mask for the corresponding position is set to 0.\n",
    "        mask = mask * (y != out_vocab.token_to_idx[EOS])\n",
    "    return l / num_not_pad_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training function, we need to update the model parameters of the encoder and the decoder at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "188"
    }
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, lr, batch_size, num_epochs):\n",
    "    print(\"Training:\")\n",
    "    encoder.initialize(init.Xavier(), force_reinit=True)\n",
    "    decoder.initialize(init.Xavier(), force_reinit=True)\n",
    "    enc_trainer = gluon.Trainer(encoder.collect_params(), 'adam',\n",
    "                                {'learning_rate': lr})\n",
    "    dec_trainer = gluon.Trainer(decoder.collect_params(), 'adam',\n",
    "                                {'learning_rate': lr})\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum = 0\n",
    "        for X, Y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = batch_loss(encoder, decoder, X, Y, loss)\n",
    "            l.backward()\n",
    "            enc_trainer.step(1)\n",
    "            dec_trainer.step(1)\n",
    "            l_sum += l.asscalar()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"epoch %d, loss %.3f\" % (epoch + 1, l_sum / len(data_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a model instance and set hyper-parameters. Then, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hardmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder initialized\n",
      "Decoder initialized\n",
      "Training:\n",
      "epoch 10, loss 3.888\n",
      "epoch 20, loss 3.536\n",
      "epoch 30, loss 3.369\n",
      "epoch 40, loss 3.343\n",
      "epoch 50, loss 3.245\n"
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers = 64, 64, 2\n",
    "attention_size, drop_prob, lr, batch_size, num_epochs = 10, 0.5, 0.01, 2, 50\n",
    "encoder = Encoder(len(in_vocab), embed_size, num_hiddens, num_layers,\n",
    "                  drop_prob)\n",
    "decoder = Decoder(len(out_vocab), embed_size, num_hiddens, num_layers,\n",
    "                  attention_size, drop_prob)\n",
    "train(encoder, decoder, dataset, lr, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder initialized\n",
      "Decoder initialized\n",
      "Hardmax used:False\n",
      "Training:\n",
      "epoch 10, loss 3.431\n",
      "epoch 20, loss 3.120\n",
      "epoch 30, loss 3.104\n",
      "epoch 40, loss 3.049\n",
      "epoch 50, loss 3.009\n"
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers = 64, 64, 2\n",
    "attention_size, drop_prob, lr, batch_size, num_epochs = 10, 0.5, 0.01, 2, 50\n",
    "encoder = Encoder(len(in_vocab), embed_size, num_hiddens, num_layers,\n",
    "                  drop_prob)\n",
    "decoder = Decoder(len(out_vocab), embed_size, num_hiddens, num_layers,\n",
    "                  attention_size, drop_prob, False)\n",
    "train(encoder, decoder, dataset, lr, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
